{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T04:34:13.545579Z","iopub.status.busy":"2024-09-20T04:34:13.545074Z","iopub.status.idle":"2024-09-20T04:34:20.878031Z","shell.execute_reply":"2024-09-20T04:34:20.877050Z","shell.execute_reply.started":"2024-09-20T04:34:13.545542Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","import torch\n","import numpy as np\n","import re\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import LongformerForSequenceClassification, LongformerTokenizerFast\n","from transformers import LongformerTokenizer, AdamW, get_linear_schedule_with_warmup\n","from torch.optim import AdamW\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from datasets import Dataset\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n","import time\n","from sklearn.model_selection import train_test_split\n","import random\n","from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n","import xgboost as xgb\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T04:34:42.185960Z","iopub.status.busy":"2024-09-20T04:34:42.184896Z","iopub.status.idle":"2024-09-20T04:34:43.711509Z","shell.execute_reply":"2024-09-20T04:34:43.710422Z","shell.execute_reply.started":"2024-09-20T04:34:42.185919Z"},"trusted":true},"outputs":[],"source":["data = pd.read_json('/kaggle/input/action-final-data/action_240307_sequence.json')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T04:34:43.714383Z","iopub.status.busy":"2024-09-20T04:34:43.713499Z","iopub.status.idle":"2024-09-20T04:34:43.725398Z","shell.execute_reply":"2024-09-20T04:34:43.724226Z","shell.execute_reply.started":"2024-09-20T04:34:43.714334Z"},"trusted":true},"outputs":[],"source":["# data split function\n","def split_data(data, sample_SEED, fixSEED=0):\n","  # normal bot split\n","  normal = data[data['restrict']==0].reset_index(drop=True)\n","  bot = data[data['restrict']==1].reset_index(drop=True)\n","  # normal underampling\n","  random.seed(sample_SEED)\n","  sample = random.sample(list(range(len(normal))), len(bot))\n","  normal = normal.loc[sample].reset_index(drop=True)\n","  data = pd.concat([normal, bot], axis=0)\n","  data['restrict'].value_counts()\n","  # train valid test split\n","  train, test = train_test_split(data, test_size=0.3, random_state=fixSEED, stratify = data['restrict'])\n","  train, valid = train_test_split(train, test_size=0.3, random_state=fixSEED, stratify = train['restrict'])\n","  train = train.reset_index(drop=True)\n","  valid = valid.reset_index(drop=True)\n","  test = test.reset_index(drop=True)\n","  # 1:1 fix\n","  random.seed(fixSEED)\n","  move = random.sample(list(valid[valid['restrict']==1].index), 1)\n","  test = pd.concat([test, valid.loc[move]], axis=0).reset_index(drop=True)\n","  valid = valid.drop(move, axis=0).reset_index(drop=True)\n","  return train, valid, test\n","\n","def extract_first_two_sentences(text):\n","    sentences = re.split(r'(?<=\\.)\\s+', text)\n","    return ' '.join(sentences[:2])"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T04:34:44.014839Z","iopub.status.busy":"2024-09-20T04:34:44.014455Z","iopub.status.idle":"2024-09-20T04:34:44.029490Z","shell.execute_reply":"2024-09-20T04:34:44.028703Z","shell.execute_reply.started":"2024-09-20T04:34:44.014802Z"},"trusted":true},"outputs":[],"source":["from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T04:34:45.553006Z","iopub.status.busy":"2024-09-20T04:34:45.552377Z","iopub.status.idle":"2024-09-20T04:34:45.566184Z","shell.execute_reply":"2024-09-20T04:34:45.565337Z","shell.execute_reply.started":"2024-09-20T04:34:45.552950Z"},"trusted":true},"outputs":[],"source":["# Seed 설정\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# Training function\n","def train(model, train_loader, optimizer, scheduler):\n","    model.train()\n","    total_loss = 0\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","    for step, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n","        with torch.cuda.amp.autocast():\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['restrict'].to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss / gradient_accumulation_steps\n","            total_loss += loss.item()\n","\n","        scaler.scale(loss).backward()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            scheduler.step()\n","            optimizer.zero_grad()\n","\n","    return total_loss / len(train_loader)\n","\n","# Evaluation function\n","def evaluate(model, data_loader):\n","    model.eval()\n","    preds, true_labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['restrict'].to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            preds.extend(torch.argmax(logits, axis=1).cpu().numpy())\n","            true_labels.extend(labels.cpu().numpy())\n","\n","    return preds, true_labels\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T04:34:47.422916Z","iopub.status.busy":"2024-09-20T04:34:47.422518Z","iopub.status.idle":"2024-09-20T04:34:47.432787Z","shell.execute_reply":"2024-09-20T04:34:47.431848Z","shell.execute_reply.started":"2024-09-20T04:34:47.422877Z"},"trusted":true},"outputs":[],"source":["# Tokenization function\n","def tokenize_function(examples, key, max_length):\n","    return tokenizer(examples[key], padding='max_length', truncation=True, max_length=max_length)\n","\n","# Preprocessing function\n","def prepare_data(df, key, max_length):\n","    data = {key: df[key].tolist(), 'restrict': df['restrict'].tolist()}\n","    dataset = Dataset.from_dict(data)\n","    dataset = dataset.map(lambda x: tokenize_function(x, key, max_length), batched=True)\n","    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'restrict'])\n","    return dataset\n","\n","# Dataloader function\n","def dataloader(df_train, df_valid, df_test, variable):\n","    # Define the variable-to-key mapping\n","    key_map = {\n","        'text': ('text', 512),\n","        'longtext': ('longtext', 512),\n","        'longlongtext': ('longlongtext', 512),\n","        'base': ('base', 31)\n","    }\n","    \n","    # Retrieve the appropriate key and max_length for the variable\n","    key, max_length = key_map[variable]\n","    \n","    # Prepare datasets\n","    train_dataset = prepare_data(df_train, key, max_length)\n","    val_dataset = prepare_data(df_valid, key, max_length)\n","    test_dataset = prepare_data(df_test, key, max_length)\n","    \n","    # Create DataLoaders\n","    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=8)\n","    test_loader = DataLoader(test_dataset, batch_size=8)\n","    \n","    return train_loader, val_loader, test_loader\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# SEED = 5"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-12T05:41:49.182310Z","iopub.status.busy":"2024-09-12T05:41:49.181859Z","iopub.status.idle":"2024-09-12T05:41:49.901305Z","shell.execute_reply":"2024-09-12T05:41:49.900486Z","shell.execute_reply.started":"2024-09-12T05:41:49.182270Z"},"trusted":true},"outputs":[],"source":["# train valid test split\n","df_train, df_valid, df_test = split_data(data, 5)\n","\n","# Apply the function to the text_column and create a new column\n","df_train['base'] = df_train['longlongtext'].apply(extract_first_two_sentences)\n","df_valid['base'] = df_valid['longlongtext'].apply(extract_first_two_sentences)\n","df_test['base'] = df_test['longlongtext'].apply(extract_first_two_sentences)"]},{"cell_type":"markdown","metadata":{},"source":["## text5"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-12T05:41:51.755964Z","iopub.status.busy":"2024-09-12T05:41:51.755569Z","iopub.status.idle":"2024-09-12T06:11:20.144582Z","shell.execute_reply":"2024-09-12T06:11:20.143511Z","shell.execute_reply.started":"2024-09-12T05:41:51.755911Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"739a63e76df74b398a308c50d0f99b9b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5642 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"055e2e046e254de19a5e470610245717","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2418 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"005ecb08d1cd4e0a879cb050ea2873e2","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3456 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n","Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.08022137066460196\n","Epoch 1 Training Time: 281.75 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.37it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Validation F1 Score: 0.7211815561959656\n","Validation Precision: 0.6388002552648373\n","Validation Recall: 0.8279569892473119\n","Validation Accuracy: 0.6799007444168734\n","Validation Confusion Matrix:\n"," [[ 643  566]\n"," [ 208 1001]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 0.07427663303299599\n","Epoch 2 Training Time: 281.60 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.38it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Validation F1 Score: 0.7114467408585056\n","Validation Precision: 0.6847742922723795\n","Validation Recall: 0.7402812241521919\n","Validation Accuracy: 0.6997518610421837\n","Validation Confusion Matrix:\n"," [[797 412]\n"," [314 895]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 0.07333799529683489\n","Epoch 3 Training Time: 281.69 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.37it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Validation F1 Score: 0.7322920450417726\n","Validation Precision: 0.6528497409326425\n","Validation Recall: 0.8337468982630273\n","Validation Accuracy: 0.69520264681555\n","Validation Confusion Matrix:\n"," [[ 673  536]\n"," [ 201 1008]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 0.07131375874743583\n","Epoch 4 Training Time: 281.67 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.37it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Validation F1 Score: 0.7405611361274679\n","Validation Precision: 0.6370679380214541\n","Validation Recall: 0.8842018196856907\n","Validation Accuracy: 0.6902398676592225\n","Validation Confusion Matrix:\n"," [[ 600  609]\n"," [ 140 1069]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 0.07083960633102288\n","Epoch 5 Training Time: 281.60 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Validation F1 Score: 0.7393364928909952\n","Validation Precision: 0.6610169491525424\n","Validation Recall: 0.8387096774193549\n","Validation Accuracy: 0.7043010752688172\n","Validation Confusion Matrix:\n"," [[ 689  520]\n"," [ 195 1014]]\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 432/432 [00:51<00:00,  8.39it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Results:\n","Test F1 Score: 0.7377731529656606\n","Test Precision: 0.6701323251417769\n","Test Recall: 0.8206018518518519\n","Test Accuracy: 0.7083333333333334\n","Test Confusion Matrix:\n"," [[1030  698]\n"," [ 310 1418]]\n","Total Training Time: 1589.33 seconds\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Bert model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","train_loader, val_loader, test_loader = dataloader(df_train, df_valid, df_test, 'text')\n","\n","# Optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","num_epochs = 5\n","\n","# Learning rate scheduler\n","num_training_steps = num_epochs * len(train_loader)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","gradient_accumulation_steps = 8\n","seed = 0\n","set_seed(seed)\n","\n","\n","# 훈련 시간 측정 시작\n","start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    epoch_start_time = time.time()\n","    train_loss = train(model, train_loader, optimizer, scheduler)\n","    epoch_end_time = time.time()\n","\n","    print(f\"Epoch {epoch + 1}, Loss: {train_loss}\")\n","    print(f\"Epoch {epoch + 1} Training Time: {epoch_end_time - epoch_start_time:.2f} seconds\")\n","\n","    # validation data 평가\n","    predicted_labels, true_labels = evaluate(model, val_loader)\n","    f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","    precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","    recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","    print(f\"Epoch {epoch + 1}, Validation F1 Score: {f1}\")\n","    print(f\"Validation Precision: {precision}\")\n","    print(f\"Validation Recall: {recall}\")\n","    print(f\"Validation Accuracy: {accuracy}\")\n","    print(\"Validation Confusion Matrix:\\n\", conf_matrix)\n","\n","\n","# 훈련 시간 측정 종료\n","end_time = time.time()\n","total_training_time = end_time - start_time # 훈련시간\n","\n","# Final prediction\n","predicted_labels, true_labels = evaluate(model, test_loader)\n","f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","print(\"Final Test Results:\")\n","print(f\"Test F1 Score: {f1}\")\n","print(f\"Test Precision: {precision}\")\n","print(f\"Test Recall: {recall}\")\n","print(f\"Test Accuracy: {accuracy}\")\n","print(\"Test Confusion Matrix:\\n\", conf_matrix)\n","print(f\"Total Training Time: {total_training_time:.2f} seconds\")"]},{"cell_type":"markdown","metadata":{},"source":["## longtext5"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-12T06:11:41.688085Z","iopub.status.busy":"2024-09-12T06:11:41.687644Z","iopub.status.idle":"2024-09-12T06:41:14.876843Z","shell.execute_reply":"2024-09-12T06:41:14.875778Z","shell.execute_reply.started":"2024-09-12T06:11:41.688043Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebcb3a4a8f2841ccbbd5ad6ab6677e5d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5642 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"959f3f1ff99d48c2bf1e67af1e115927","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2418 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d273e5ac9644a11a2e7a43290003189","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3456 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n","Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.08056996091567085\n","Epoch 1 Training Time: 281.67 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.38it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Validation F1 Score: 0.7258123402701716\n","Validation Precision: 0.6496732026143791\n","Validation Recall: 0.8221670802315963\n","Validation Accuracy: 0.6894127377998346\n","Validation Confusion Matrix:\n"," [[673 536]\n"," [215 994]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 0.0742372595217005\n","Epoch 2 Training Time: 281.67 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.38it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Validation F1 Score: 0.7008750994431185\n","Validation Precision: 0.675095785440613\n","Validation Recall: 0.728701406120761\n","Validation Accuracy: 0.6889991728701406\n","Validation Confusion Matrix:\n"," [[785 424]\n"," [328 881]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 0.07307075846296532\n","Epoch 3 Training Time: 281.68 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.37it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Validation F1 Score: 0.738037865748709\n","Validation Precision: 0.6320754716981132\n","Validation Recall: 0.8866832092638545\n","Validation Accuracy: 0.6852770885028949\n","Validation Confusion Matrix:\n"," [[ 585  624]\n"," [ 137 1072]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 0.07095441196863105\n","Epoch 4 Training Time: 281.80 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.37it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Validation F1 Score: 0.7344064386317909\n","Validation Precision: 0.6175972927241963\n","Validation Recall: 0.9057071960297767\n","Validation Accuracy: 0.6724565756823822\n","Validation Confusion Matrix:\n"," [[ 531  678]\n"," [ 114 1095]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 0.06992039329269452\n","Epoch 5 Training Time: 281.81 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Validation F1 Score: 0.7390988372093025\n","Validation Precision: 0.6591056383668179\n","Validation Recall: 0.8411910669975186\n","Validation Accuracy: 0.7030603804797353\n","Validation Confusion Matrix:\n"," [[ 683  526]\n"," [ 192 1017]]\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 432/432 [00:51<00:00,  8.38it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Results:\n","Test F1 Score: 0.7335406946604458\n","Test Precision: 0.6643192488262911\n","Test Recall: 0.8188657407407407\n","Test Accuracy: 0.7025462962962963\n","Test Confusion Matrix:\n"," [[1013  715]\n"," [ 313 1415]]\n","Total Training Time: 1589.56 seconds\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Bert model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","train_loader, val_loader, test_loader = dataloader(df_train, df_valid, df_test, 'longtext')\n","\n","# Optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","num_epochs = 5\n","\n","# Learning rate scheduler\n","num_training_steps = num_epochs * len(train_loader)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","gradient_accumulation_steps = 8\n","seed = 0\n","set_seed(seed)\n","\n","\n","# 훈련 시간 측정 시작\n","start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    epoch_start_time = time.time()\n","    train_loss = train(model, train_loader, optimizer, scheduler)\n","    epoch_end_time = time.time()\n","\n","    print(f\"Epoch {epoch + 1}, Loss: {train_loss}\")\n","    print(f\"Epoch {epoch + 1} Training Time: {epoch_end_time - epoch_start_time:.2f} seconds\")\n","\n","    # validation data 평가\n","    predicted_labels, true_labels = evaluate(model, val_loader)\n","    f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","    precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","    recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","    print(f\"Epoch {epoch + 1}, Validation F1 Score: {f1}\")\n","    print(f\"Validation Precision: {precision}\")\n","    print(f\"Validation Recall: {recall}\")\n","    print(f\"Validation Accuracy: {accuracy}\")\n","    print(\"Validation Confusion Matrix:\\n\", conf_matrix)\n","\n","\n","# 훈련 시간 측정 종료\n","end_time = time.time()\n","total_training_time = end_time - start_time # 훈련시간\n","\n","# Final prediction\n","predicted_labels, true_labels = evaluate(model, test_loader)\n","f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","print(\"Final Test Results:\")\n","print(f\"Test F1 Score: {f1}\")\n","print(f\"Test Precision: {precision}\")\n","print(f\"Test Recall: {recall}\")\n","print(f\"Test Accuracy: {accuracy}\")\n","print(\"Test Confusion Matrix:\\n\", conf_matrix)\n","print(f\"Total Training Time: {total_training_time:.2f} seconds\")"]},{"cell_type":"markdown","metadata":{},"source":["## longlongtext5"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-12T06:41:19.529391Z","iopub.status.busy":"2024-09-12T06:41:19.529015Z","iopub.status.idle":"2024-09-12T07:10:56.543677Z","shell.execute_reply":"2024-09-12T07:10:56.542607Z","shell.execute_reply.started":"2024-09-12T06:41:19.529353Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e8e7f31b0ca48899a7d70223e245cef","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5642 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1be4c668891f47ba82a90a870b960788","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2418 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94462db91b5b48fbbc8d166cea451dff","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3456 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n","Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.08068086607935746\n","Epoch 1 Training Time: 281.74 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.38it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Validation F1 Score: 0.7117516629711751\n","Validation Precision: 0.6432865731462926\n","Validation Recall: 0.7965260545905707\n","Validation Accuracy: 0.6774193548387096\n","Validation Confusion Matrix:\n"," [[675 534]\n"," [246 963]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 0.0750864031632291\n","Epoch 2 Training Time: 281.76 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.37it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Validation F1 Score: 0.6921241050119331\n","Validation Precision: 0.6666666666666666\n","Validation Recall: 0.7196029776674938\n","Validation Accuracy: 0.6799007444168734\n","Validation Confusion Matrix:\n"," [[774 435]\n"," [339 870]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 0.07395072294024503\n","Epoch 3 Training Time: 281.78 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.36it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Validation F1 Score: 0.7339514978601998\n","Validation Precision: 0.645141065830721\n","Validation Recall: 0.8511166253101737\n","Validation Accuracy: 0.6914805624483044\n","Validation Confusion Matrix:\n"," [[ 643  566]\n"," [ 180 1029]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 0.0715086487129795\n","Epoch 4 Training Time: 281.79 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.41it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Validation F1 Score: 0.7352941176470588\n","Validation Precision: 0.6375227686703097\n","Validation Recall: 0.8684863523573201\n","Validation Accuracy: 0.6873449131513648\n","Validation Confusion Matrix:\n"," [[ 612  597]\n"," [ 159 1050]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [04:41<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 0.07081844448705571\n","Epoch 5 Training Time: 281.53 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:36<00:00,  8.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Validation F1 Score: 0.7292048369366069\n","Validation Precision: 0.6546052631578947\n","Validation Recall: 0.8229942100909843\n","Validation Accuracy: 0.6943755169561621\n","Validation Confusion Matrix:\n"," [[684 525]\n"," [214 995]]\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 432/432 [00:51<00:00,  8.39it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Results:\n","Test F1 Score: 0.7313120752744381\n","Test Precision: 0.6668255481410867\n","Test Recall: 0.8096064814814815\n","Test Accuracy: 0.7025462962962963\n","Test Confusion Matrix:\n"," [[1029  699]\n"," [ 329 1399]]\n","Total Training Time: 1589.47 seconds\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Bert model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","train_loader, val_loader, test_loader = dataloader(df_train, df_valid, df_test, 'longlongtext')\n","\n","# Optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","num_epochs = 5\n","\n","# Learning rate scheduler\n","num_training_steps = num_epochs * len(train_loader)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","gradient_accumulation_steps = 8\n","seed = 0\n","set_seed(seed)\n","\n","\n","# 훈련 시간 측정 시작\n","start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    epoch_start_time = time.time()\n","    train_loss = train(model, train_loader, optimizer, scheduler)\n","    epoch_end_time = time.time()\n","\n","    print(f\"Epoch {epoch + 1}, Loss: {train_loss}\")\n","    print(f\"Epoch {epoch + 1} Training Time: {epoch_end_time - epoch_start_time:.2f} seconds\")\n","\n","    # validation data 평가\n","    predicted_labels, true_labels = evaluate(model, val_loader)\n","    f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","    precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","    recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","    print(f\"Epoch {epoch + 1}, Validation F1 Score: {f1}\")\n","    print(f\"Validation Precision: {precision}\")\n","    print(f\"Validation Recall: {recall}\")\n","    print(f\"Validation Accuracy: {accuracy}\")\n","    print(\"Validation Confusion Matrix:\\n\", conf_matrix)\n","\n","\n","# 훈련 시간 측정 종료\n","end_time = time.time()\n","total_training_time = end_time - start_time # 훈련시간\n","\n","# Final prediction\n","predicted_labels, true_labels = evaluate(model, test_loader)\n","f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","print(\"Final Test Results:\")\n","print(f\"Test F1 Score: {f1}\")\n","print(f\"Test Precision: {precision}\")\n","print(f\"Test Recall: {recall}\")\n","print(f\"Test Accuracy: {accuracy}\")\n","print(\"Test Confusion Matrix:\\n\", conf_matrix)\n","print(f\"Total Training Time: {total_training_time:.2f} seconds\")"]},{"cell_type":"markdown","metadata":{},"source":["## base5"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-12T07:11:11.962338Z","iopub.status.busy":"2024-09-12T07:11:11.961913Z","iopub.status.idle":"2024-09-12T07:14:25.725143Z","shell.execute_reply":"2024-09-12T07:14:25.724174Z","shell.execute_reply.started":"2024-09-12T07:11:11.962299Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ced781f4add543bdbc85fe9df1c5d4a6","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5642 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b970bf9f222d48e7b14513b3e4c0433b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2418 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cc6feb4c2004ebf8ae416203b23fa72","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3456 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n","Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [00:30<00:00, 23.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.08318476096091122\n","Epoch 1 Training Time: 30.61 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:04<00:00, 74.85it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Validation F1 Score: 0.6820208023774145\n","Validation Precision: 0.6190155091031693\n","Validation Recall: 0.7593052109181141\n","Validation Accuracy: 0.6459884201819686\n","Validation Confusion Matrix:\n"," [[644 565]\n"," [291 918]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [00:30<00:00, 23.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 0.07960184032450993\n","Epoch 2 Training Time: 30.68 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:04<00:00, 75.37it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Validation F1 Score: 0.7122736418511066\n","Validation Precision: 0.5989847715736041\n","Validation Recall: 0.8784119106699751\n","Validation Accuracy: 0.6451612903225806\n","Validation Confusion Matrix:\n"," [[ 498  711]\n"," [ 147 1062]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [00:30<00:00, 22.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 0.0788536058269209\n","Epoch 3 Training Time: 30.82 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:04<00:00, 74.53it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Validation F1 Score: 0.7124492557510149\n","Validation Precision: 0.6027475672581568\n","Validation Recall: 0.8709677419354839\n","Validation Accuracy: 0.6484698097601324\n","Validation Confusion Matrix:\n"," [[ 515  694]\n"," [ 156 1053]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [00:30<00:00, 22.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 0.07846196447485905\n","Epoch 4 Training Time: 30.82 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:04<00:00, 75.74it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Validation F1 Score: 0.7142857142857143\n","Validation Precision: 0.5987688864017907\n","Validation Recall: 0.8850289495450786\n","Validation Accuracy: 0.6459884201819686\n","Validation Confusion Matrix:\n"," [[ 492  717]\n"," [ 139 1070]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/706 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 706/706 [00:30<00:00, 22.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 0.07819248123817336\n","Epoch 5 Training Time: 30.77 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 303/303 [00:04<00:00, 75.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Validation F1 Score: 0.7131886477462437\n","Validation Precision: 0.5979843225083986\n","Validation Recall: 0.8833746898263027\n","Validation Accuracy: 0.6447477253928867\n","Validation Confusion Matrix:\n"," [[ 491  718]\n"," [ 141 1068]]\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 432/432 [00:05<00:00, 75.35it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Results:\n","Test F1 Score: 0.7113772455089821\n","Test Precision: 0.6068655496526358\n","Test Recall: 0.859375\n","Test Accuracy: 0.6513310185185185\n","Test Confusion Matrix:\n"," [[ 766  962]\n"," [ 243 1485]]\n","Total Training Time: 173.98 seconds\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Bert model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","train_loader, val_loader, test_loader = dataloader(df_train, df_valid, df_test, 'base')\n","\n","# Optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","num_epochs = 5\n","\n","# Learning rate scheduler\n","num_training_steps = num_epochs * len(train_loader)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","gradient_accumulation_steps = 8\n","seed = 0\n","set_seed(seed)\n","\n","\n","# 훈련 시간 측정 시작\n","start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    epoch_start_time = time.time()\n","    train_loss = train(model, train_loader, optimizer, scheduler)\n","    epoch_end_time = time.time()\n","\n","    print(f\"Epoch {epoch + 1}, Loss: {train_loss}\")\n","    print(f\"Epoch {epoch + 1} Training Time: {epoch_end_time - epoch_start_time:.2f} seconds\")\n","\n","    # validation data 평가\n","    predicted_labels, true_labels = evaluate(model, val_loader)\n","    f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","    precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","    recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","    print(f\"Epoch {epoch + 1}, Validation F1 Score: {f1}\")\n","    print(f\"Validation Precision: {precision}\")\n","    print(f\"Validation Recall: {recall}\")\n","    print(f\"Validation Accuracy: {accuracy}\")\n","    print(\"Validation Confusion Matrix:\\n\", conf_matrix)\n","\n","\n","# 훈련 시간 측정 종료\n","end_time = time.time()\n","total_training_time = end_time - start_time # 훈련시간\n","\n","# Final prediction\n","predicted_labels, true_labels = evaluate(model, test_loader)\n","f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","print(\"Final Test Results:\")\n","print(f\"Test F1 Score: {f1}\")\n","print(f\"Test Precision: {precision}\")\n","print(f\"Test Recall: {recall}\")\n","print(f\"Test Accuracy: {accuracy}\")\n","print(\"Test Confusion Matrix:\\n\", conf_matrix)\n","print(f\"Total Training Time: {total_training_time:.2f} seconds\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5684028,"sourceId":9371836,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
