{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T00:55:44.676458Z","iopub.status.busy":"2024-09-17T00:55:44.675842Z","iopub.status.idle":"2024-09-17T00:55:52.118192Z","shell.execute_reply":"2024-09-17T00:55:52.117279Z","shell.execute_reply.started":"2024-09-17T00:55:44.676417Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","import torch\n","import numpy as np\n","import re\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import LongformerForSequenceClassification, LongformerTokenizerFast\n","from transformers import LongformerTokenizer, AdamW, get_linear_schedule_with_warmup\n","from torch.optim import AdamW\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from datasets import Dataset\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n","import time\n","from sklearn.model_selection import train_test_split\n","import random\n","from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n","import xgboost as xgb\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T00:55:52.120872Z","iopub.status.busy":"2024-09-17T00:55:52.120233Z","iopub.status.idle":"2024-09-17T00:55:54.187267Z","shell.execute_reply":"2024-09-17T00:55:54.186461Z","shell.execute_reply.started":"2024-09-17T00:55:52.120824Z"},"trusted":true},"outputs":[],"source":["data = pd.read_json('/kaggle/input/action-data-0307-final/action_240307_sequence.json')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T00:55:54.188692Z","iopub.status.busy":"2024-09-17T00:55:54.188395Z","iopub.status.idle":"2024-09-17T00:55:54.198600Z","shell.execute_reply":"2024-09-17T00:55:54.197581Z","shell.execute_reply.started":"2024-09-17T00:55:54.188660Z"},"trusted":true},"outputs":[],"source":["# data split function\n","def split_data(data, sample_SEED, fixSEED=0):\n","  # normal bot split\n","  normal = data[data['restrict']==0].reset_index(drop=True)\n","  bot = data[data['restrict']==1].reset_index(drop=True)\n","  # normal underampling\n","  random.seed(sample_SEED)\n","  sample = random.sample(list(range(len(normal))), len(bot))\n","  normal = normal.loc[sample].reset_index(drop=True)\n","  data = pd.concat([normal, bot], axis=0)\n","  data['restrict'].value_counts()\n","  # train valid test split\n","  train, test = train_test_split(data, test_size=0.3, random_state=fixSEED, stratify = data['restrict'])\n","  train, valid = train_test_split(train, test_size=0.3, random_state=fixSEED, stratify = train['restrict'])\n","  train = train.reset_index(drop=True)\n","  valid = valid.reset_index(drop=True)\n","  test = test.reset_index(drop=True)\n","  # 1:1 fix\n","  random.seed(fixSEED)\n","  move = random.sample(list(valid[valid['restrict']==1].index), 1)\n","  test = pd.concat([test, valid.loc[move]], axis=0).reset_index(drop=True)\n","  valid = valid.drop(move, axis=0).reset_index(drop=True)\n","  return train, valid, test\n","\n","def extract_first_two_sentences(text):\n","    sentences = re.split(r'(?<=\\.)\\s+', text)\n","    return ' '.join(sentences[:2])"]},{"cell_type":"markdown","metadata":{},"source":["# SEED = 5"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T02:31:34.080265Z","iopub.status.busy":"2024-09-14T02:31:34.079221Z","iopub.status.idle":"2024-09-14T02:31:34.822900Z","shell.execute_reply":"2024-09-14T02:31:34.821883Z","shell.execute_reply.started":"2024-09-14T02:31:34.080202Z"},"trusted":true},"outputs":[],"source":["# data split function\n","def split_data(data, sample_SEED, fixSEED=0):\n","  # normal bot split\n","  normal = data[data['restrict']==0].reset_index(drop=True)\n","  bot = data[data['restrict']==1].reset_index(drop=True)\n","  # normal underampling\n","  random.seed(sample_SEED)\n","  sample = random.sample(list(range(len(normal))), len(bot))\n","  normal = normal.loc[sample].reset_index(drop=True)\n","  data = pd.concat([normal, bot], axis=0)\n","  data['restrict'].value_counts()\n","  # train valid test split\n","  train, test = train_test_split(data, test_size=0.3, random_state=fixSEED, stratify = data['restrict'])\n","  train, valid = train_test_split(train, test_size=0.3, random_state=fixSEED, stratify = train['restrict'])\n","  train = train.reset_index(drop=True)\n","  valid = valid.reset_index(drop=True)\n","  test = test.reset_index(drop=True)\n","  # 1:1 fix\n","  random.seed(fixSEED)\n","  move = random.sample(list(valid[valid['restrict']==1].index), 1)\n","  test = pd.concat([test, valid.loc[move]], axis=0).reset_index(drop=True)\n","  valid = valid.drop(move, axis=0).reset_index(drop=True)\n","  return train, valid, test\n","\n","def extract_first_two_sentences(text):\n","    sentences = re.split(r'(?<=\\.)\\s+', text)\n","    return ' '.join(sentences[:2])\n","\n","\n","# train valid test split\n","df_train, df_valid, df_test = split_data(data, 5)\n","\n","# Apply the function to the text_column and create a new column\n","df_train['base'] = df_train['longlongtext'].apply(extract_first_two_sentences)\n","df_valid['base'] = df_valid['longlongtext'].apply(extract_first_two_sentences)\n","df_test['base'] = df_test['longlongtext'].apply(extract_first_two_sentences)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-14T02:31:35.849255Z","iopub.status.busy":"2024-09-14T02:31:35.848555Z","iopub.status.idle":"2024-09-14T02:31:35.869060Z","shell.execute_reply":"2024-09-14T02:31:35.867992Z","shell.execute_reply.started":"2024-09-14T02:31:35.849211Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>base_ymd</th>\n","      <th>playerid</th>\n","      <th>text</th>\n","      <th>longtext</th>\n","      <th>longlongtext</th>\n","      <th>restrict</th>\n","      <th>base</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2024-03-07</td>\n","      <td>3a205556d558613d4042345066961b67abee8eee0bb3bb...</td>\n","      <td>The user performed the action 'warp_teleport_n...</td>\n","      <td>The maximum number of consecutive actions is 1...</td>\n","      <td>The count of unique action type is 7. Maximum,...</td>\n","      <td>0</td>\n","      <td>The count of unique action type is 7. Maximum,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2024-03-07</td>\n","      <td>0487d4e85bcb1a95daab158ec1f74cec19208862d690d9...</td>\n","      <td>The user performed the action 'item collection...</td>\n","      <td>The maximum number of consecutive actions is 1...</td>\n","      <td>The count of unique action type is 8. Maximum,...</td>\n","      <td>1</td>\n","      <td>The count of unique action type is 8. Maximum,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2024-03-07</td>\n","      <td>8d79d204afbc4dae19451aa064ad6fc8b7974630dd2830...</td>\n","      <td>The user performed the action 'warp_teleport_n...</td>\n","      <td>The maximum number of consecutive actions is 4...</td>\n","      <td>The count of unique action type is 2. Maximum,...</td>\n","      <td>1</td>\n","      <td>The count of unique action type is 2. Maximum,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2024-03-07</td>\n","      <td>4fe53a78282425ab8d06c9a9f4802a4c421c2a6c072790...</td>\n","      <td>The user performed the action 'stat_set_monste...</td>\n","      <td>The maximum number of consecutive actions is 3...</td>\n","      <td>The count of unique action type is 12. Maximum...</td>\n","      <td>1</td>\n","      <td>The count of unique action type is 12. Maximum...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2024-03-07</td>\n","      <td>c7be3a2578338258c2d06681da01859423801e6ffbc36c...</td>\n","      <td>The user performed the action 'warp_teleport_n...</td>\n","      <td>The maximum number of consecutive actions is 7...</td>\n","      <td>The count of unique action type is 7. Maximum,...</td>\n","      <td>0</td>\n","      <td>The count of unique action type is 7. Maximum,...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5637</th>\n","      <td>2024-03-07</td>\n","      <td>23d7d6896692f1e04ea407c896fdb78cfc2d02a0179bf0...</td>\n","      <td>The user performed the action 'warp_teleport_n...</td>\n","      <td>The maximum number of consecutive actions is 1...</td>\n","      <td>The count of unique action type is 10. Maximum...</td>\n","      <td>0</td>\n","      <td>The count of unique action type is 10. Maximum...</td>\n","    </tr>\n","    <tr>\n","      <th>5638</th>\n","      <td>2024-03-07</td>\n","      <td>62b0c9cd10e22ea5ed2206f9de9acf9a5efb73428a4961...</td>\n","      <td>The user performed the action 'warp_teleport_n...</td>\n","      <td>The maximum number of consecutive actions is 8...</td>\n","      <td>The count of unique action type is 4. Maximum,...</td>\n","      <td>1</td>\n","      <td>The count of unique action type is 4. Maximum,...</td>\n","    </tr>\n","    <tr>\n","      <th>5639</th>\n","      <td>2024-03-07</td>\n","      <td>c34c077635dffbc156483f491eba041fa794c388a8a1b5...</td>\n","      <td>The user performed the action 'warp_teleport_n...</td>\n","      <td>The maximum number of consecutive actions is 6...</td>\n","      <td>The count of unique action type is 5. Maximum,...</td>\n","      <td>0</td>\n","      <td>The count of unique action type is 5. Maximum,...</td>\n","    </tr>\n","    <tr>\n","      <th>5640</th>\n","      <td>2024-03-07</td>\n","      <td>f19e7c0d44726ce3a075d8c2275b59d5f16e3041414e07...</td>\n","      <td>The user performed the action 'warp_teleport_n...</td>\n","      <td>The maximum number of consecutive actions is 8...</td>\n","      <td>The count of unique action type is 7. Maximum,...</td>\n","      <td>1</td>\n","      <td>The count of unique action type is 7. Maximum,...</td>\n","    </tr>\n","    <tr>\n","      <th>5641</th>\n","      <td>2024-03-07</td>\n","      <td>624571c9d6327ca1eb0c448352a11bba2d52c218e739a0...</td>\n","      <td>The user performed the action 'stat_set_stat s...</td>\n","      <td>The maximum number of consecutive actions is 2...</td>\n","      <td>The count of unique action type is 25. Maximum...</td>\n","      <td>0</td>\n","      <td>The count of unique action type is 25. Maximum...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5642 rows × 7 columns</p>\n","</div>"],"text/plain":["        base_ymd                                           playerid  \\\n","0     2024-03-07  3a205556d558613d4042345066961b67abee8eee0bb3bb...   \n","1     2024-03-07  0487d4e85bcb1a95daab158ec1f74cec19208862d690d9...   \n","2     2024-03-07  8d79d204afbc4dae19451aa064ad6fc8b7974630dd2830...   \n","3     2024-03-07  4fe53a78282425ab8d06c9a9f4802a4c421c2a6c072790...   \n","4     2024-03-07  c7be3a2578338258c2d06681da01859423801e6ffbc36c...   \n","...          ...                                                ...   \n","5637  2024-03-07  23d7d6896692f1e04ea407c896fdb78cfc2d02a0179bf0...   \n","5638  2024-03-07  62b0c9cd10e22ea5ed2206f9de9acf9a5efb73428a4961...   \n","5639  2024-03-07  c34c077635dffbc156483f491eba041fa794c388a8a1b5...   \n","5640  2024-03-07  f19e7c0d44726ce3a075d8c2275b59d5f16e3041414e07...   \n","5641  2024-03-07  624571c9d6327ca1eb0c448352a11bba2d52c218e739a0...   \n","\n","                                                   text  \\\n","0     The user performed the action 'warp_teleport_n...   \n","1     The user performed the action 'item collection...   \n","2     The user performed the action 'warp_teleport_n...   \n","3     The user performed the action 'stat_set_monste...   \n","4     The user performed the action 'warp_teleport_n...   \n","...                                                 ...   \n","5637  The user performed the action 'warp_teleport_n...   \n","5638  The user performed the action 'warp_teleport_n...   \n","5639  The user performed the action 'warp_teleport_n...   \n","5640  The user performed the action 'warp_teleport_n...   \n","5641  The user performed the action 'stat_set_stat s...   \n","\n","                                               longtext  \\\n","0     The maximum number of consecutive actions is 1...   \n","1     The maximum number of consecutive actions is 1...   \n","2     The maximum number of consecutive actions is 4...   \n","3     The maximum number of consecutive actions is 3...   \n","4     The maximum number of consecutive actions is 7...   \n","...                                                 ...   \n","5637  The maximum number of consecutive actions is 1...   \n","5638  The maximum number of consecutive actions is 8...   \n","5639  The maximum number of consecutive actions is 6...   \n","5640  The maximum number of consecutive actions is 8...   \n","5641  The maximum number of consecutive actions is 2...   \n","\n","                                           longlongtext  restrict  \\\n","0     The count of unique action type is 7. Maximum,...         0   \n","1     The count of unique action type is 8. Maximum,...         1   \n","2     The count of unique action type is 2. Maximum,...         1   \n","3     The count of unique action type is 12. Maximum...         1   \n","4     The count of unique action type is 7. Maximum,...         0   \n","...                                                 ...       ...   \n","5637  The count of unique action type is 10. Maximum...         0   \n","5638  The count of unique action type is 4. Maximum,...         1   \n","5639  The count of unique action type is 5. Maximum,...         0   \n","5640  The count of unique action type is 7. Maximum,...         1   \n","5641  The count of unique action type is 25. Maximum...         0   \n","\n","                                                   base  \n","0     The count of unique action type is 7. Maximum,...  \n","1     The count of unique action type is 8. Maximum,...  \n","2     The count of unique action type is 2. Maximum,...  \n","3     The count of unique action type is 12. Maximum...  \n","4     The count of unique action type is 7. Maximum,...  \n","...                                                 ...  \n","5637  The count of unique action type is 10. Maximum...  \n","5638  The count of unique action type is 4. Maximum,...  \n","5639  The count of unique action type is 5. Maximum,...  \n","5640  The count of unique action type is 7. Maximum,...  \n","5641  The count of unique action type is 25. Maximum...  \n","\n","[5642 rows x 7 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_train"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T12:16:04.772059Z","iopub.status.busy":"2024-09-11T12:16:04.771672Z","iopub.status.idle":"2024-09-11T12:16:06.416849Z","shell.execute_reply":"2024-09-11T12:16:06.415930Z","shell.execute_reply.started":"2024-09-11T12:16:04.772024Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["LongformerForSequenceClassification(\n","  (longformer): LongformerModel(\n","    (embeddings): LongformerEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n","    )\n","    (encoder): LongformerEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): LongformerClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# Longformer model, tokenizer\n","tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n","model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels=2)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T00:56:08.815952Z","iopub.status.busy":"2024-09-17T00:56:08.815086Z","iopub.status.idle":"2024-09-17T00:56:08.827773Z","shell.execute_reply":"2024-09-17T00:56:08.826721Z","shell.execute_reply.started":"2024-09-17T00:56:08.815912Z"},"trusted":true},"outputs":[],"source":["# Seed 설정\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# Training function\n","def train(model, train_loader, optimizer, scheduler):\n","    model.train()\n","    total_loss = 0\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","    for step, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n","        with torch.cuda.amp.autocast():\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['restrict'].to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss / gradient_accumulation_steps\n","            total_loss += loss.item()\n","\n","        scaler.scale(loss).backward()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            scheduler.step()\n","            optimizer.zero_grad()\n","\n","    return total_loss / len(train_loader)\n","\n","# Evaluation function\n","def evaluate(model, data_loader):\n","    model.eval()\n","    preds, true_labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['restrict'].to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            preds.extend(torch.argmax(logits, axis=1).cpu().numpy())\n","            true_labels.extend(labels.cpu().numpy())\n","\n","    return preds, true_labels\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T00:56:09.391818Z","iopub.status.busy":"2024-09-17T00:56:09.390790Z","iopub.status.idle":"2024-09-17T00:56:09.401197Z","shell.execute_reply":"2024-09-17T00:56:09.400201Z","shell.execute_reply.started":"2024-09-17T00:56:09.391774Z"},"trusted":true},"outputs":[],"source":["# Tokenization function\n","def tokenize_function(examples, key, max_length):\n","    return tokenizer(examples[key], padding='max_length', truncation=True, max_length=max_length)\n","\n","# Preprocessing function\n","def prepare_data(df, key, max_length):\n","    data = {key: df[key].tolist(), 'restrict': df['restrict'].tolist()}\n","    dataset = Dataset.from_dict(data)\n","    dataset = dataset.map(lambda x: tokenize_function(x, key, max_length), batched=True)\n","    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'restrict'])\n","    return dataset\n","\n","# Dataloader function\n","def dataloader(df_train, df_valid, df_test, variable):\n","    # Define the variable-to-key mapping\n","    key_map = {\n","        'text': ('text', 1024),\n","        'longtext': ('longtext', 1024),\n","        'longlongtext': ('longlongtext', 1024),\n","        'base': ('base', 31)\n","    }\n","    \n","    # Retrieve the appropriate key and max_length for the variable\n","    key, max_length = key_map[variable]\n","    \n","    # Prepare datasets\n","    train_dataset = prepare_data(df_train, key, max_length)\n","    val_dataset = prepare_data(df_valid, key, max_length)\n","    test_dataset = prepare_data(df_test, key, max_length)\n","    \n","    # Create DataLoaders\n","    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=1)\n","    test_loader = DataLoader(test_dataset, batch_size=1)\n","    \n","    return train_loader, val_loader, test_loader\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Text5"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T12:19:05.335702Z","iopub.status.busy":"2024-09-11T12:19:05.335297Z","iopub.status.idle":"2024-09-11T12:19:50.630447Z","shell.execute_reply":"2024-09-11T12:19:50.629648Z","shell.execute_reply.started":"2024-09-11T12:19:05.335666Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d385fc93d4b44c86992f24088732c122","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5642 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc4c7910f5084911910553e4097d2d7b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2418 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eeb33f030d214736801cab519bdeec11","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3456 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_loader, val_loader, test_loader = dataloader(df_train, df_valid, df_test, 'text')\n","\n","\n","# Optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","num_epochs = 2\n","\n","# Learning rate scheduler\n","num_training_steps = num_epochs * len(train_loader)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","gradient_accumulation_steps = 8\n","seed = 0\n","set_seed(seed)"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T12:19:53.424534Z","iopub.status.busy":"2024-09-11T12:19:53.423772Z","iopub.status.idle":"2024-09-11T13:22:03.181922Z","shell.execute_reply":"2024-09-11T13:22:03.180821Z","shell.execute_reply.started":"2024-09-11T12:19:53.424497Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n","Training:   0%|          | 0/5642 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 5642/5642 [25:10<00:00,  3.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.08162665003709275\n","Epoch 1 Training Time: 1510.91 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 2418/2418 [03:25<00:00, 11.75it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Validation F1 Score: 0.7268258426966292\n","Validation Precision: 0.6314826113483831\n","Validation Recall: 0.8560794044665012\n","Validation Accuracy: 0.6782464846980976\n","Validation Confusion Matrix:\n"," [[ 605  604]\n"," [ 174 1035]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/5642 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 5642/5642 [25:11<00:00,  3.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 0.07635324012299287\n","Epoch 2 Training Time: 1511.47 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 2418/2418 [03:27<00:00, 11.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Validation F1 Score: 0.7290540540540541\n","Validation Precision: 0.6162193032552827\n","Validation Recall: 0.8924731182795699\n","Validation Accuracy: 0.6683209263854425\n","Validation Confusion Matrix:\n"," [[ 537  672]\n"," [ 130 1079]]\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 3456/3456 [04:54<00:00, 11.74it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Results:\n","Test F1 Score: 0.7303803562831006\n","Test Precision: 0.6253091508656224\n","Test Recall: 0.8778935185185185\n","Test Accuracy: 0.6759259259259259\n","Test Confusion Matrix:\n"," [[ 819  909]\n"," [ 211 1517]]\n","Total Training Time: 3435.31 seconds\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["num_epochs = 2\n","\n","seed = 0\n","set_seed(seed)\n","\n","# 훈련 시간 측정 시작\n","start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    epoch_start_time = time.time()\n","    train_loss = train(model, train_loader, optimizer, scheduler)\n","    epoch_end_time = time.time()\n","\n","    print(f\"Epoch {epoch + 1}, Loss: {train_loss}\")\n","    print(f\"Epoch {epoch + 1} Training Time: {epoch_end_time - epoch_start_time:.2f} seconds\")\n","\n","    # validation data 평가\n","    predicted_labels, true_labels = evaluate(model, val_loader)\n","    f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","    precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","    recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","    print(f\"Epoch {epoch + 1}, Validation F1 Score: {f1}\")\n","    print(f\"Validation Precision: {precision}\")\n","    print(f\"Validation Recall: {recall}\")\n","    print(f\"Validation Accuracy: {accuracy}\")\n","    print(\"Validation Confusion Matrix:\\n\", conf_matrix)\n","\n","\n","# 훈련 시간 측정 종료\n","end_time = time.time()\n","total_training_time = end_time - start_time # 훈련시간\n","\n","# Final prediction\n","predicted_labels, true_labels = evaluate(model, test_loader)\n","f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","print(\"Final Test Results:\")\n","print(f\"Test F1 Score: {f1}\")\n","print(f\"Test Precision: {precision}\")\n","print(f\"Test Recall: {recall}\")\n","print(f\"Test Accuracy: {accuracy}\")\n","print(\"Test Confusion Matrix:\\n\", conf_matrix)\n","print(f\"Total Training Time: {total_training_time:.2f} seconds\")"]},{"cell_type":"markdown","metadata":{},"source":["## longtext5"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T13:23:25.477422Z","iopub.status.busy":"2024-09-11T13:23:25.476542Z","iopub.status.idle":"2024-09-11T14:26:27.331488Z","shell.execute_reply":"2024-09-11T14:26:27.330382Z","shell.execute_reply.started":"2024-09-11T13:23:25.477383Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9b50860e0ef4bb0ada993b7c2850682","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5642 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"575ba3efaffa440fbed5269058bab4b6","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2418 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cea0914a5c6141adaa9724b4801209ea","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3456 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n","Training:   0%|          | 0/5642 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 5642/5642 [25:15<00:00,  3.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.08222267180628406\n","Epoch 1 Training Time: 1515.43 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 2418/2418 [03:26<00:00, 11.68it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Validation F1 Score: 0.6989835809225957\n","Validation Precision: 0.6627131208302446\n","Validation Recall: 0.739454094292804\n","Validation Accuracy: 0.6815550041356493\n","Validation Confusion Matrix:\n"," [[754 455]\n"," [315 894]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/5642 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 5642/5642 [25:10<00:00,  3.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 0.07714815285137383\n","Epoch 2 Training Time: 1510.87 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 2418/2418 [03:25<00:00, 11.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Validation F1 Score: 0.733837689133425\n","Validation Precision: 0.6280164802825191\n","Validation Recall: 0.8825475599669148\n","Validation Accuracy: 0.6799007444168734\n","Validation Confusion Matrix:\n"," [[ 577  632]\n"," [ 142 1067]]\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 3456/3456 [04:53<00:00, 11.77it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Results:\n","Test F1 Score: 0.7335623159960745\n","Test Precision: 0.6367120954003407\n","Test Recall: 0.8651620370370371\n","Test Accuracy: 0.6857638888888888\n","Test Confusion Matrix:\n"," [[ 875  853]\n"," [ 233 1495]]\n","Total Training Time: 3439.01 seconds\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Longformer model, tokenizer\n","tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n","model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels=2)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","train_loader, val_loader, test_loader = dataloader(df_train, df_valid, df_test, 'longtext')\n","\n","# Optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","num_epochs = 2\n","\n","# Learning rate scheduler\n","num_training_steps = num_epochs * len(train_loader)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","gradient_accumulation_steps = 8\n","seed = 0\n","set_seed(seed)\n","\n","num_epochs = 2\n","\n","# 훈련 시간 측정 시작\n","start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    epoch_start_time = time.time()\n","    train_loss = train(model, train_loader, optimizer, scheduler)\n","    epoch_end_time = time.time()\n","\n","    print(f\"Epoch {epoch + 1}, Loss: {train_loss}\")\n","    print(f\"Epoch {epoch + 1} Training Time: {epoch_end_time - epoch_start_time:.2f} seconds\")\n","\n","    # validation data 평가\n","    predicted_labels, true_labels = evaluate(model, val_loader)\n","    f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","    precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","    recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","    print(f\"Epoch {epoch + 1}, Validation F1 Score: {f1}\")\n","    print(f\"Validation Precision: {precision}\")\n","    print(f\"Validation Recall: {recall}\")\n","    print(f\"Validation Accuracy: {accuracy}\")\n","    print(\"Validation Confusion Matrix:\\n\", conf_matrix)\n","\n","\n","# 훈련 시간 측정 종료\n","end_time = time.time()\n","total_training_time = end_time - start_time # 훈련시간\n","\n","# Final prediction\n","predicted_labels, true_labels = evaluate(model, test_loader)\n","f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","print(\"Final Test Results:\")\n","print(f\"Test F1 Score: {f1}\")\n","print(f\"Test Precision: {precision}\")\n","print(f\"Test Recall: {recall}\")\n","print(f\"Test Accuracy: {accuracy}\")\n","print(\"Test Confusion Matrix:\\n\", conf_matrix)\n","print(f\"Total Training Time: {total_training_time:.2f} seconds\")"]},{"cell_type":"markdown","metadata":{},"source":["## longlongtext5"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T15:31:34.404595Z","iopub.status.busy":"2024-09-11T15:31:34.404145Z","iopub.status.idle":"2024-09-11T16:34:42.659233Z","shell.execute_reply":"2024-09-11T16:34:42.658183Z","shell.execute_reply.started":"2024-09-11T15:31:34.404555Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6307bd3fd0044a9195ab14cc08035d13","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5642 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"106b8275b34e4b1d9a25918a4a3d8a78","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2418 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ebf795428f3459d869b45c11310d074","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3456 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n","Training:   0%|          | 0/5642 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 5642/5642 [25:10<00:00,  3.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.08377091827987568\n","Epoch 1 Training Time: 1510.39 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 2418/2418 [03:26<00:00, 11.70it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Validation F1 Score: 0.7194139194139194\n","Validation Precision: 0.6456278763971072\n","Validation Recall: 0.8122415219189413\n","Validation Accuracy: 0.6832092638544252\n","Validation Confusion Matrix:\n"," [[670 539]\n"," [227 982]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/5642 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 5642/5642 [25:16<00:00,  3.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 0.07694968108968589\n","Epoch 2 Training Time: 1516.17 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 2418/2418 [03:28<00:00, 11.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Validation F1 Score: 0.7345529858474283\n","Validation Precision: 0.6303317535545023\n","Validation Recall: 0.880066170388751\n","Validation Accuracy: 0.6819685690653433\n","Validation Confusion Matrix:\n"," [[ 585  624]\n"," [ 145 1064]]\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 3456/3456 [04:57<00:00, 11.62it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Results:\n","Test F1 Score: 0.734633423845964\n","Test Precision: 0.6405510116229014\n","Test Recall: 0.8611111111111112\n","Test Accuracy: 0.6889467592592593\n","Test Confusion Matrix:\n"," [[ 893  835]\n"," [ 240 1488]]\n","Total Training Time: 3441.29 seconds\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Longformer model, tokenizer\n","tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n","model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels=2)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","train_loader, val_loader, test_loader = dataloader(df_train, df_valid, df_test, 'longlongtext')\n","\n","# Optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","num_epochs = 2\n","\n","# Learning rate scheduler\n","num_training_steps = num_epochs * len(train_loader)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","gradient_accumulation_steps = 8\n","seed = 0\n","set_seed(seed)\n","\n","num_epochs = 2\n","\n","# 훈련 시간 측정 시작\n","start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    epoch_start_time = time.time()\n","    train_loss = train(model, train_loader, optimizer, scheduler)\n","    epoch_end_time = time.time()\n","\n","    print(f\"Epoch {epoch + 1}, Loss: {train_loss}\")\n","    print(f\"Epoch {epoch + 1} Training Time: {epoch_end_time - epoch_start_time:.2f} seconds\")\n","\n","    # validation data 평가\n","    predicted_labels, true_labels = evaluate(model, val_loader)\n","    f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","    precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","    recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","    print(f\"Epoch {epoch + 1}, Validation F1 Score: {f1}\")\n","    print(f\"Validation Precision: {precision}\")\n","    print(f\"Validation Recall: {recall}\")\n","    print(f\"Validation Accuracy: {accuracy}\")\n","    print(\"Validation Confusion Matrix:\\n\", conf_matrix)\n","\n","\n","# 훈련 시간 측정 종료\n","end_time = time.time()\n","total_training_time = end_time - start_time # 훈련시간\n","\n","# Final prediction\n","predicted_labels, true_labels = evaluate(model, test_loader)\n","f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","print(\"Final Test Results:\")\n","print(f\"Test F1 Score: {f1}\")\n","print(f\"Test Precision: {precision}\")\n","print(f\"Test Recall: {recall}\")\n","print(f\"Test Accuracy: {accuracy}\")\n","print(\"Test Confusion Matrix:\\n\", conf_matrix)\n","print(f\"Total Training Time: {total_training_time:.2f} seconds\")"]},{"cell_type":"markdown","metadata":{},"source":["## Base5"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T16:34:50.173813Z","iopub.status.busy":"2024-09-11T16:34:50.173028Z","iopub.status.idle":"2024-09-11T17:17:02.428596Z","shell.execute_reply":"2024-09-11T17:17:02.427542Z","shell.execute_reply.started":"2024-09-11T16:34:50.173757Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"581f1d7526fe4b19923905aa96a30b20","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5642 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db09ef461512461e851c8173b7b5867a","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2418 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"787374c0d48d4b989e56fe5210811053","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3456 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n","Training:   0%|          | 0/5642 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n","Training: 100%|██████████| 5642/5642 [16:55<00:00,  5.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.08204483064680562\n","Epoch 1 Training Time: 1015.04 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 2418/2418 [02:26<00:00, 16.50it/s]\n","/tmp/ipykernel_36/324861209.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Validation F1 Score: 0.7029840388619014\n","Validation Precision: 0.6054991034070532\n","Validation Recall: 0.8378825475599669\n","Validation Accuracy: 0.6459884201819686\n","Validation Confusion Matrix:\n"," [[ 549  660]\n"," [ 196 1013]]\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/5642 [00:00<?, ?it/s]/tmp/ipykernel_36/324861209.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 5642/5642 [16:45<00:00,  5.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 0.08067244158193432\n","Epoch 2 Training Time: 1005.58 seconds\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 2418/2418 [02:25<00:00, 16.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Validation F1 Score: 0.7086113759135685\n","Validation Precision: 0.5753353973168215\n","Validation Recall: 0.9222497932175352\n","Validation Accuracy: 0.6207609594706369\n","Validation Confusion Matrix:\n"," [[ 386  823]\n"," [  94 1115]]\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 3456/3456 [03:27<00:00, 16.68it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Results:\n","Test F1 Score: 0.7099253900067827\n","Test Precision: 0.5825602968460112\n","Test Recall: 0.9085648148148148\n","Test Accuracy: 0.6287615740740741\n","Test Confusion Matrix:\n"," [[ 603 1125]\n"," [ 158 1570]]\n","Total Training Time: 2312.88 seconds\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Longformer model, tokenizer\n","tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n","model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels=2)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","train_loader, val_loader, test_loader = dataloader(df_train, df_valid, df_test, 'base')\n","\n","# Optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","num_epochs = 2\n","\n","# Learning rate scheduler\n","num_training_steps = num_epochs * len(train_loader)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","gradient_accumulation_steps = 8\n","seed = 0\n","set_seed(seed)\n","\n","num_epochs = 2\n","\n","# 훈련 시간 측정 시작\n","start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    epoch_start_time = time.time()\n","    train_loss = train(model, train_loader, optimizer, scheduler)\n","    epoch_end_time = time.time()\n","\n","    print(f\"Epoch {epoch + 1}, Loss: {train_loss}\")\n","    print(f\"Epoch {epoch + 1} Training Time: {epoch_end_time - epoch_start_time:.2f} seconds\")\n","\n","    # validation data 평가\n","    predicted_labels, true_labels = evaluate(model, val_loader)\n","    f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","    precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","    recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","    print(f\"Epoch {epoch + 1}, Validation F1 Score: {f1}\")\n","    print(f\"Validation Precision: {precision}\")\n","    print(f\"Validation Recall: {recall}\")\n","    print(f\"Validation Accuracy: {accuracy}\")\n","    print(\"Validation Confusion Matrix:\\n\", conf_matrix)\n","\n","\n","# 훈련 시간 측정 종료\n","end_time = time.time()\n","total_training_time = end_time - start_time # 훈련시간\n","\n","# Final prediction\n","predicted_labels, true_labels = evaluate(model, test_loader)\n","f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n","precision = precision_score(true_labels, predicted_labels, zero_division=1)\n","recall = recall_score(true_labels, predicted_labels, zero_division=1)\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","\n","print(\"Final Test Results:\")\n","print(f\"Test F1 Score: {f1}\")\n","print(f\"Test Precision: {precision}\")\n","print(f\"Test Recall: {recall}\")\n","print(f\"Test Accuracy: {accuracy}\")\n","print(\"Test Confusion Matrix:\\n\", conf_matrix)\n","print(f\"Total Training Time: {total_training_time:.2f} seconds\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5680289,"sourceId":9366903,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
